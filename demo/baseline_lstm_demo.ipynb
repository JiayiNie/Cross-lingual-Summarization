{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zeNXkas2-rCl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/niejiayi/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pickle \n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from io import open\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "%matplotlib inline\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "833dGkU00CaI"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9x74wMa-zUO0"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=1, drop_prob=0):\n",
        "        super(EncoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        #Embed input words\n",
        "        embedded = self.embedding(inputs)\n",
        "        #Pass the embedded word vectors into LSTM and return all outputs\n",
        "        output, hidden = self.lstm(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvOYqFSl0GjW"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YrNC0oKC0PON"
      },
      "outputs": [],
      "source": [
        "class LuongDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, attention, n_layers=1, drop_prob=0.1):\n",
        "        super(LuongDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "        #Our Attention Mechanism is defined in a separate class\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.drop_prob)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        self.classifier = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "    \n",
        "    def forward(self, inputs, hidden, encoder_outputs):\n",
        "        #Embed input words\n",
        "        embedded = self.embedding(inputs).view(1,1,-1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        #Passing previous output word (embedded) and hidden state into LSTM cell\n",
        "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
        "\n",
        "        #Calculating Alignment Scores - see Attention class for the forward pass function\n",
        "        alignment_scores = self.attention(hidden[0], encoder_outputs)\n",
        "        #Softmaxing alignment scores to obtain Attention weights\n",
        "        attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
        "\n",
        "        #Multiplying Attention weights with encoder outputs to get context vector\n",
        "        context_vector = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs)\n",
        "\n",
        "        #Concatenating output from LSTM with context vector\n",
        "        output = torch.cat((lstm_out, context_vector),-1)\n",
        "        #Pass concatenated vector through Linear layer acting as a Classifier\n",
        "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vkLkxfTPzz-e"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, method=\"dot\"):\n",
        "        super(Attention, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #Defining the layers/weights required depending on alignment scoring method\n",
        "        if method == \"general\":\n",
        "            self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        elif method == \"concat\":\n",
        "            self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
        "  \n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        if self.method == \"dot\":\n",
        "          #For the dot scoring method, no weights or linear layers are involved\n",
        "          return encoder_outputs.bmm(decoder_hidden.view(1,-1,1)).squeeze(-1)\n",
        "    \n",
        "        elif self.method == \"general\":\n",
        "            #For general scoring, decoder hidden state is passed through linear layers to introduce a weight matrix\n",
        "            out = self.fc(decoder_hidden)\n",
        "            return encoder_outputs.bmm(out.view(1,-1,1)).squeeze(-1)\n",
        "\n",
        "        elif self.method == \"concat\":\n",
        "            #For concat scoring, decoder hidden state and encoder outputs are concatenated first\n",
        "            out = torch.tanh(self.fc(decoder_hidden+encoder_outputs))\n",
        "            return out.bmm(self.weight.unsqueeze(-1)).squeeze(-1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RvuejzcKxUgC"
      },
      "source": [
        "#DEMO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OHf9PRU6u2hx"
      },
      "outputs": [],
      "source": [
        "#load dictionaries\n",
        "path_to_dic = \"../util/vocabulary_lstm/\"\n",
        "with open(path_to_dic+'en_word2index.pkl', 'rb') as f:\n",
        "    loaded_en_w2i = pickle.load(f)\n",
        "with open(path_to_dic+'en_index2word.pkl', 'rb') as f:\n",
        "    loaded_en_i2w = pickle.load(f)\n",
        "\n",
        "with open(path_to_dic+'fr_word2index.pkl', 'rb') as f:\n",
        "    loaded_fr_w2i = pickle.load(f)\n",
        "with open(path_to_dic+'fr_index2word.pkl', 'rb') as f:\n",
        "    loaded_fr_i2w = pickle.load(f)\n",
        "\n",
        "\n",
        "#load test samples\n",
        "path_to_data = \"../data/\"\n",
        "with open(path_to_data+'fr_text_preprocessed_test1000.txt', encoding='utf-8') as file3:\n",
        "    test_articles = file3.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "32utCOJmDZHW"
      },
      "outputs": [],
      "source": [
        "fr_inputs = []\n",
        "\n",
        "#Converting French testing articles to their token indexes\n",
        "for i in range(len(test_articles)):\n",
        "\n",
        "    #tokenize first:\n",
        "    fr_tokens = word_tokenize(test_articles[i])\n",
        "    fr_inputs.append([token.lower() for token in fr_tokens] + ['_EOS'])\n",
        "    \n",
        "    #word to index\n",
        "    fr_article= fr_inputs[i]\n",
        "    fr_inputs[i] = [loaded_fr_w2i[word] if word in loaded_fr_w2i else loaded_fr_w2i['_UNK'] for word in fr_article]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEX-mp2e9MLj",
        "outputId": "80424267-4e00-4cca-99cc-6f1738c62cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  1000 testing samples\n",
            "Visualize one article for testing: separation pouvoirs notion plus celebree nen guere daussi confuse 1789 declaration droits lhomme citoyen notait article quune societe laquelle separation pouvoirs netait garantie point constitution deux siecles plus tard quasi-totalite constitutions adoptees pays lex-bloc sovietique reprenaient presque mot mot meme enonce faits nul pourtant jamais envisage trois pouvoirs executif judiciaire legislatif puisque cest deux quil sagissait puissent fonctionner facon vraiment autonome derriere terme separation cest fait plutot lidee dun equilibre dune balance pouvoirs lon envisagee paradoxe regimes moins democratiques entendu sappuyer xixe siecle lexpression separation pouvoirs _UNK exemple lindependance pouvoir executif mettre labri toute velleite controle parlementaire cela ete cas second empire france concentrons-nous present posant deux questions avons-nous besoin dune separation dun equilibre pouvoirs oui selon quelles modalites notons preambule vieille _UNK plus aucun sens toutes societes modernes partout quun seul pouvoir dirigeant pouvoir executif cest reviennent initiatives decisions essentielles pouvoir legislatif selon modalites differentes autres pays quune capacite limitee controler contraindre voire censurer lexecutif quant pouvoir judiciaire nexiste plus depuis longtemps tant tel notion navait sens lorsque systeme judiciaire participait lexercice volonte politique fonction legislative meme executive avant mette place etat administratif activite etant dorenavant contentieuse cest dautorite judiciaire quil convient parler terme separation pouvoirs selon lancienne _UNK donc plus consistance nen moins plus necessaire jamais contrecarrer tendance permanente pouvoir general executif sexercer sans contrepoids presenter comme seul legitime contre cette double pretention necessaire reformuler termes dune nouvelle architecture pouvoirs plus dune separation dune balance ceux-ci cest termes complication demultiplication distinction fonctions formes democratiques quil faut raisonner faut dabord demultiplier modes dexpression volonte generale pouvoir politique tire legitimite lelection celle-ci mele deux dimensions _EOS\n"
          ]
        }
      ],
      "source": [
        "print(\"There are \",len(fr_inputs), \"testing samples\")\n",
        "print(\"Visualize one article for testing: \" + \" \".join([loaded_fr_i2w[x] for x in fr_inputs[10]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQvy0e7A0fP",
        "outputId": "3cdf3113-5afc-4c5c-e0bf-c7984ac54e77"
      },
      "outputs": [],
      "source": [
        "hidden_size = 512\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_path = \"../weights/model_epoch_5_lstm.pth\"\n",
        "\n",
        "# Load the saved model\n",
        "checkpoint = torch.load(model_path)\n",
        "\n",
        "# Extract the encoder and decoder state dictionaries from the loaded checkpoint\n",
        "encoder_state_dict = checkpoint[\"encoder\"]\n",
        "decoder_state_dict = checkpoint[\"decoder\"]\n",
        "\n",
        "# Create the encoder and decoder models with the same architecture as the saved models\n",
        "encoder = EncoderLSTM(len(loaded_fr_i2w), hidden_size).to(device)\n",
        "attn = Attention(hidden_size,\"concat\")\n",
        "decoder = LuongDecoder(hidden_size,len(loaded_en_i2w),attn).to(device)\n",
        "\n",
        "# Load the state dictionaries into the models\n",
        "encoder.load_state_dict(encoder_state_dict)\n",
        "decoder.load_state_dict(decoder_state_dict)\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0wJFZKR0nEe"
      },
      "outputs": [],
      "source": [
        "path_to_evaluation = \"../evaluation/baseline_lstm/\"\n",
        "with open(path_to_evaluation+\"demo.txt\", \"w\") as f:\n",
        "  for idx in range(0,len(fr_inputs)):\n",
        "      h = encoder.init_hidden()\n",
        "      inp = torch.tensor(fr_inputs[idx]).unsqueeze(0).to(device)\n",
        "      encoder_outputs, h = encoder(inp,h)\n",
        "\n",
        "      decoder_input = torch.tensor([loaded_en_w2i['_SOS']],device=device)\n",
        "      decoder_hidden = h\n",
        "      output = []\n",
        "      attentions = []\n",
        "      cnt = 0\n",
        "      while True:\n",
        "          decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "          _, top_index = decoder_output.topk(2)\n",
        "          decoder_input = torch.tensor([top_index[0][1].item()],device=device)\n",
        "          #If the decoder output is the End Of Sentence token, stop decoding process\n",
        "          if top_index[0][1].item() == loaded_fr_w2i[\"_EOS\"] or cnt >=25:\n",
        "              break\n",
        "          output.append(top_index[0][1].item())\n",
        "          attentions.append(attn_weights.squeeze().cpu().detach().numpy())\n",
        "          cnt+=1\n",
        "\n",
        "      summary = ' '.join([loaded_en_i2w[x] for x in output])\n",
        "      print(summary)\n",
        "      f.write(summary + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEGNNI4VBwaZ",
        "outputId": "d678cbaf-79d0-4838-8d66-956f0fb856f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in paris of a simple term are now department , is a simple term , where he was the victim . the government . .\n"
          ]
        }
      ],
      "source": [
        "with open(path_to_evaluation+'demo.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    lines = [line.strip() for line in lines]  # Remove trailing newline characters\n",
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieHJ9Jy7VopR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
